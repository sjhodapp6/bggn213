---
title: "Machine Learning 1"
author: 'Stefanie Hodapp (PID: A53300084)'
date: "10/22/2021"
output:
  pdf_document: default
  html_document: default
---

# Clustering methods

Kmeans clustering in R is done with the 'Kmeans()' function

```{r}
tmp <- c(rnorm(30, 3), rnorm(30, -3) )
data <- cbind(x=tmp, y=rev(tmp))
plot(data)
```

Run 'kmeans()' and set k (centers) to 2 (i.e. the number of clusters we want), nstart to 20 (to run multiple times). You have to tell it how many clusters you want. Clustering vector tells you which cluster each element in your data set is in. 

```{r}
km <- kmeans(data, centers=2, nstart=20)
km
```
> Q. How many points are in each cluster?

```{r}
km$size
```
> Q. What 'component' of your result object details cluster assignment/membership?

```{r}
km$cluster
```

> Q. What 'component' of your result object details cluster center?

```{r}
km$centers
```

> Q. Plot x colored by the kmeans cluster assignment and add cluster centers as blue points

```{r}
plot(data, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=2)
```
# Hierarchical Clustering

We will used the 'hclust()' function on the same data as before and see how this method works. 

```{r}
hc <- hclust(dist(data))
hc
```
hclust has a plot method

```{r}
plot(hc)
abline(h=7, col="red")
```
To find our membership vector we need to "cut" the tree and for this we use the 'cutree()' function and tell it the height to cut at. 

```{r}
cutree(hc, h=7)
```
We can also use 'cutree()' and state the number of clusters we want...

```{r}
grps <- cutree(hc, k=2)
```

```{r}
plot(data,col=grps)
```

# Principal Component Analysis (PCA)

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
```

```{r}
nrow(x)
ncol(x)
```
```{r}
head(x)
```
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
x
```
```{r}
barplot(as.matrix(x), col=rainbow(17))
```
```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```
```{r}
mycols <- rainbow(nrow(x))
pairs(x, col=mycols, pch=16)
```
```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)
```
## PCA to the rescue!

Here we will use the base R function for PCA, which is called 'prcomp()'. 'prcomp()' expects the observations to be rows and the variables to be columns therefore we need to first transpose our data.frame matrix with the t() transpose function.Use the 't()' for this.

```{r}
pca <- prcomp(t(x))
summary(pca)
```

```{r}
pca
```

```{r}
attributes(pca)
```
Plot PCA 1 vs PCA 2

```{r}
plot(pca$x[,1:2])
text(pca$x[,1:2], labels=colnames(x))
```
We can also examine the PCA "loadings", which tell us how much the original variables contribute to each new PC...

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot(pca$rotation[,1], las=2)
```

PCA of RNA sequencing data

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

```{r}
pca_rna <- prcomp(t(rna.data), scale=TRUE)
plot(pca_rna$x[,1], pca_rna$x[,2], xlab="PC1", ylab="PC2")
```
```{r}
summary(pca_rna)
```
Calling the plot function on our prcomp data will show which PC captures the most variance in our data. 

```{r}
plot(pca_rna, main="Quick scree plot")

# Most of our variability is in PC 1
```

```{r}
## Variance captured per PC 
pca.var <- pca_rna$sdev^2

## Percent variance is often more informative to look at 
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
```

```{r}
colvec <- colnames(rna.data)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

plot(pca_rna$x[,1], pca_rna$x[,2], col=colvec, pch=16,
     xlab=paste0("PC1 (", pca.var.per[1], "%)"),
     ylab=paste0("PC2 (", pca.var.per[2], "%)"))

text(pca_rna$x[,1], pca_rna$x[,2], labels = colnames(rna.data), pos=c(rep(4,5), rep(2,5)))
```



